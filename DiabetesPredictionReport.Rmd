---
title: "Machine learning predictive models to classify Pima Indians Diabetes dataset"
author: "Jordi Toneu"
date: "18/5/2020"
output: pdf_document
---

```{r setup, include=FALSE}

# Setup parameters by default
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning = FALSE)

```
This project is the second exercice of the HarvardX: PH125.9x Data Science Capstone course. Each student has to freely choose a dataset, define the target of the project, apply different machine learning techniques and properly communicate the process and insights gained from the dataset analysis.  

Dedicated to my loved children Oriol and Maria, which well deserved more attention from my side while I was passionately inmmersed with this Harvard  Professional Certificate Program in Data Science during the 2020 lockdown in The Netherlands.  
\     
\  

## Index


1.1 Overview  
1.2 The target  
1.3 The dataset   
2.1 Data exploration and cleaning  
2.2 Data normalization  
2.3 Data splitting  
2.4 Modelling approaches  
2.4.1 Logistic regression model  
2.4.2 KNN model  
2.4.3 CART model  
2.4.4 Random Forest model  
3.1 Results  
3.2 Discussion  
4. Conclusions  

\  
\  
\  
\  
\  
\  
\  
\newpage

## 1.1 Overview

Diabetes is a chronic disease that causes high sugar level in blood. Over the time, it can damage organs like heart and kidneys. Some consequences are blindness, heart attacks, stroke, kidney failure or partial amputation.

We will consider different approaches to develop a supervised machine learning algorithm to classify the risk to be diabetic, given independent features.   


## 1.2 The target

The target is to evaluate four different predictive models performance to predict the risk to be diabetic. The techniques we will use to build the four models are: logistic regression, KNN, CART and Random Forest.  

We will use the metric **overall accuracy** to assess models performance.     

  
## 1.3 The dataset

The **Pima Indians Diabetes dataset** was provided by the National Institute of Diabetes and Digestive and Kidney Diseases.
The dataset can be found on the Keggel website. It is a csv file. It can also be downloaded from **<https://github.com/jorditoneu/Diabetes_Project/blob/master/diabetes.csv>** or automatically download it by using the following code in **R**.   

```{r process to download the dataset, echo= TRUE}

###########################
# Downloading the dataset #
###########################

# Installing required packages
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")

# Diabetes dataset source
urlfile="https://raw.githubusercontent.com/jorditoneu/Diabetes_Project/master/diabetes.csv"

# Reading the Pima Indians Diabetes csv file
diabetes<-read_csv(url(urlfile))

```
\newpage   

## 2.1 Data exploration and cleaning
The dataset contains 9 variables, 8 predictors and 1 categorical outcome with 2 classes, and 768 observations.

```{r dataset structure}

#################################
# Data exploration and cleaning #
#################################

# Structure of the dataset
glimpse (diabetes)

```


We can have a look at the 10 first rows of the dataset.  

```{r Look at the first 10 rows}

# Printout of the table with the first 10 rows of the dataset
knitr::kable (diabetes[1:10,], table.attr = "style='width:70%;'")

```

\  
\  
   
      
      
This is the meaning of each variable.
```{r Variables table, fig.height=7}

# Creating the vector with variables name
Variable <- names(diabetes)

# Writting the vector with variables description
Description <- c("Number of times pregnant","Plasma glucose concentration a 2 hours in an oral glucose tolerance test","Diastolic blood pressure (mm Hg)","Triceps skin fold thickness (mm)","2-Hour serum insulin (mu U/ml)","Body mass index (weight in kg/(height in m)^2)","Diabetes pedigree function", "Age (years)","class variable. 0 indicates non-diabetes, 1 indicates diabetes")

# Creating the table with variables and descriptions
VarTable<- data_frame(Variable,Description)

# Printout of the table
VarTable %>% 
  knitr::kable()   

```
 
The variable **Pregnancies** denotes that the dataset contains women information only.  
   

\newpage

The R **summary()** function provides some extra information of all the variables.

```{r Summary}

# Summary of diabetes dataset
summary(diabetes)

```
We check if there is any **NA** (Not Available, No Answer) value in the dataset.

```{r Checking NA}

# Checking the number of NA's in the dataset
sum(is.na(diabetes))

```
No NA's found in the dataset.

\newpage

\begin{center}
\textbf{Predictors histograms}
\end{center}

```{r Variable distribution with histogram,fig.height=7, fig.width=6, fig.align='center'}

# As categorical outcome, we convert the variable type to factor
diabetes$Outcome <- as.factor(diabetes$Outcome)

# Pregnancies histogram
H1 <- diabetes %>%
  ggplot(aes(Pregnancies))+
  geom_histogram(binwidth = 1,fill="darkblue", color="grey") +
  labs (x="Number of pregnancies")

# Glucose histogram
H2 <- diabetes %>%
  ggplot(aes(Glucose))+ 
  geom_histogram(binwidth = 11,fill="darkblue", color="grey")+
  labs (x="Glucose")

# Blood Pressure histogram
H3 <- diabetes %>%
  ggplot(aes(BloodPressure))+ 
  geom_histogram(binwidth = 7,fill="darkblue", color="grey")+
  labs (x="Blood pressure in mm Hg")

# Skin thickness histogram
H4 <- diabetes %>% 
  ggplot(aes(SkinThickness))+
  geom_histogram(binwidth = 5,fill="darkblue", color="grey")+
  labs (x="Skin thickness in mm")

# Insulin histogram
H5 <- diabetes %>%
  ggplot(aes(Insulin))+ 
  geom_histogram(binwidth = 45,fill="darkblue", color="grey")+
  labs (x="2-Hour serum insulin (mu U/ml)")

# BMI histogram
H6 <- diabetes %>%
  ggplot(aes(BMI))+
  geom_histogram(binwidth = 4,fill="darkblue", color="grey")+
  labs (x="BMI - Body Mass Index")

# Diabetes pedigree function histogram
H7 <- diabetes %>% 
  ggplot(aes(DiabetesPedigreeFunction))+
  geom_histogram(binwidth = 0.13,fill="darkblue", color="grey")+
  labs (x="Diabetes pedigree function")

# Age histogram
H8 <- diabetes %>% 
  ggplot(aes(Age))+ 
  geom_histogram(binwidth = 3,fill="darkblue", color="grey")+
  labs (x="Age in years")

# Histograms printout
ggarrange(H1,H2,H3,H4,H5,H6,H7, H8,
          ncol = 2, nrow = 4)

```

We found no NA in the dataset, however, we observe in the previous histograms that some predictors contain a big number of 0. These values are not outliers but incorrect observations. As an example, it is not possible to have 0 mm of skin thickness. The predictors with such problems are **Glucose**, **BloodPressure**, **SkinThickness**, **Insulin** and **BMI**.   

The proportion of rows in the dataset with 0's is
```{r Computing number of rows with 0}

# Function to detect a row with a 0
zeros <- function(x)
  {
  return((sum(diabetes[x,1:8]== 0)>0)) 
  }

# Computing the proportion of rows with at least one 0
mean(sapply(1:nrow(diabetes), zeros))

```


In the event we delete the rows with 0's, the number of observations will be reduced by 44% and that will make more difficult to predict an accurate outcome. In order to avoid this problem, we will replace the 0's for the median value of each feature.   
   
      
      
\begin{center}
\textbf{Predictors histograms and density plot without 0}
\end{center}

```{r Replacing 0 values for avg values, fig.height=7, fig.width=6, fig.align='center'}

# Pregnancies histogram and density plot without 0
# No 0's correction needed
H1 <- diabetes %>%
  ggplot(aes(Pregnancies, y=..density..))+
  geom_histogram(binwidth = 1,fill="darkblue", color="grey")+
  geom_density(col=2)+
  labs (x="Number of pregnancies")

# Glucose histogram and density plot without 0
MedGlucose <- diabetes %>%
  filter (Glucose>0) %>% 
  summarize (median(Glucose))

diabetes$Glucose <-ifelse (round(diabetes$Glucose,0) ==0, as.numeric (round(MedGlucose,0)),diabetes$Glucose)

H2 <- diabetes %>%
  ggplot(aes(Glucose,y=..density..))+
  geom_histogram(binwidth = 11,fill="darkblue", color="grey")  +
  geom_density(col=2)+labs (x="Glucose")

# Blood Pressure histogram and density plot without 0
MedBloodPressure <- diabetes %>% 
  filter (BloodPressure>0) %>% 
  summarize (median(BloodPressure))

diabetes$BloodPressure <-ifelse (diabetes$BloodPressure ==0, as.numeric (round(MedBloodPressure,0)),diabetes$BloodPressure)

H3 <- diabetes %>%
  ggplot(aes(BloodPressure,y=..density..))+
  geom_histogram(binwidth = 7,fill="darkblue", color="grey")  +
  geom_density(col=2)+
  labs (x="Blood pressure in mm Hg")

# Skin thickness histogram and density plot without 0
MedSkin <- diabetes %>%
  filter (SkinThickness>0) %>% 
  summarize (median(SkinThickness))

diabetes$SkinThickness <-ifelse (diabetes$SkinThickness ==0, as.numeric (round(MedSkin,0)),diabetes$SkinThickness )

H4 <- diabetes %>% 
  ggplot(aes(SkinThickness,y=..density..))+
  geom_histogram(binwidth = 5,fill="darkblue", color="grey")  +
  geom_density(col=2)+
  labs (x="Skin thickness in mm")

# Insulin histogram and density plot without 0
MedInsulin <- diabetes %>%
  filter (Insulin>0) %>%
  summarize (median(Insulin))

diabetes$Insulin <-ifelse (diabetes$Insulin ==0, as.numeric (round(MedInsulin,0)),diabetes$Insulin)

H5<-diabetes %>% ggplot(aes(Insulin,y=..density..))+
  geom_histogram(binwidth = 45,fill="darkblue", color="grey")  +
  geom_density(col=2)+
  labs (x="2-Hour serum insulin (mu U/ml)")

# BMI histogram and density plot without 0
MedBMI <- diabetes %>% 
  filter (BMI>0) %>%
  summarize (median(BMI))

diabetes$BMI <-ifelse (diabetes$BMI ==0, as.numeric (round(MedBMI,0)),diabetes$BMI)

H6<-diabetes %>%
  ggplot(aes(BMI,y=..density..))+
  geom_histogram(binwidth = 4,fill="darkblue", color="grey") +
  geom_density(col=2)+
  labs (x="BMI - Body Mass Index")

# Diabetes pedigree function histogram and density plot without 0
# No 0's correction needed
H7 <- diabetes %>%
  ggplot(aes(DiabetesPedigreeFunction,y=..density..))+
  geom_histogram(binwidth = 0.13,fill="darkblue", color="grey")  +
  geom_density(col=2)+
  labs (x="Diabetes pedigree function")

# Age histogram and density plot without 0
# No 0's correction needed
H8 <- diabetes %>% 
  ggplot(aes(Age,y=..density..))+ 
  geom_histogram(binwidth = 3,fill="darkblue", color="grey")+
  geom_density(col=2)+
  labs (x="Age in years")

# Printout of histograms and density plot without 0
ggarrange(H1,H2,H3,H4,H5,H6,H7, H8,
          ncol = 2, nrow = 4)

```
We can see some predictors have outliers. The most significant cases are **Insulin**, **Skin thickness**, **Diabetes pedigree function** and **BMI**.  

None of the predictors follow a normal distribution, except **Blood pressure**, but this case needs to be confirmed with further analysis.  
\   
\   


\begin{center}
\textbf{Predictors stratified by outcome}
\end{center}
```{r Number of pregnancies stratified by outcome, fig.height=2, fig.width=6, fig.align='center'}

# Pregnancies density plot
P1 <- diabetes %>% 
  ggplot(aes(Pregnancies, fill = Outcome))+ 
  geom_density(alpha= 0.6) +
  labs (x="Number of pregnancies")

# Pregnancies box plot
P2 <- diabetes %>% 
  ggplot(aes(Outcome, Pregnancies, fill = Outcome))+ 
  geom_boxplot () +
  labs (y="Pregnancies")

# Plots printout
ggarrange(P1, P2,
          ncol = 2, nrow = 1)

```
We can observe that diabetic women have a higher number of pregnancies.  

```{r Glucose stratified by outcome, fig.height=2, fig.width=6, fig.align='center'}

# Glucose density plot
P1 <- diabetes %>% 
  ggplot(aes(Glucose, fill = Outcome))+ 
  geom_density(alpha= 0.6) +
  labs (x="Glucose")

# Glucose box plot
P2 <- diabetes %>% 
  ggplot(aes(Outcome, Glucose, fill = Outcome))+ 
  geom_boxplot () +
  labs (y="Glucose")

# Plots printout
ggarrange(P1, P2,
          ncol = 2, nrow = 1)

```

In this case we can pretty well distinguish non diabetic and diabetic women by the level of glucose.   
```{r Blood pressure stratified by outcome, fig.height=2, fig.width=6, fig.align='center'}

# Blood pressure density plot
P1 <- diabetes %>% 
  ggplot(aes(BloodPressure, fill = Outcome))+ 
  geom_density(alpha= 0.6) +
  labs (x="Blood pressure in mm Hg")

# Blood pressure box plot
P2 <- diabetes %>% 
  ggplot(aes(Outcome, BloodPressure, fill = Outcome))+ 
  geom_boxplot () +
  labs (y="Blood pressure")

# Plots printout
ggarrange(P1, P2,
          ncol = 2, nrow = 1)

```
No significant difference can be observed in blood pressure level of non diabetic and diabetic women.   
```{r Skin thickness stratified by outcome, fig.height=2, fig.width=6, fig.align='center'}

# Skin thickness density plot
P1 <- diabetes %>% 
  ggplot(aes(SkinThickness, fill = Outcome))+ 
  geom_density(alpha= 0.6) +
  labs (x="Skin thickness in mm")

# Skin thickness box plot
P2 <- diabetes %>% 
  ggplot(aes(Outcome, SkinThickness, fill = Outcome))+ 
  geom_boxplot () +
  labs (y="Skin thickness")

# Plots printout
ggarrange(P1, P2,
          ncol = 2, nrow = 1)

```
Diabetic women show a slightly higher skin thickness level, but the difference is minimal.   
```{r Insulin stratified by outcome, fig.height=2, fig.width=6, fig.align='center'}

# Insulin density plot
P1 <- diabetes %>% 
  ggplot(aes(Insulin, fill = Outcome))+ 
  geom_density(alpha= 0.6) +
  labs (x="2-Hour serum insulin (mu U/ml)")

# Insulin box plot
P2 <- diabetes %>% 
  ggplot(aes(Outcome, Insulin, fill = Outcome))+ 
  geom_boxplot () +
  labs (y="Insulin (mu U/ml)")

# Plots printout
ggarrange(P1, P2,
          ncol = 2, nrow = 1)

```
We can observe the same insulin level of diabetic and non diabetic women but in case of diabetic ones, the insulin level is more stable.  
```{r BMI stratified by outcome, fig.height=2, fig.width=6, fig.align='center'}

# BMI density plot
P1 <- diabetes %>% 
  ggplot(aes(BMI, fill = Outcome))+ 
  geom_density(alpha= 0.6) +
  labs (x="BMI - Body Mass Index")

# BMI box plot
P2 <- diabetes %>% 
  ggplot(aes(Outcome, BMI, fill = Outcome))+ 
  geom_boxplot () +
  labs (y="BMI")

# Plots printout
ggarrange(P1, P2,
          ncol = 2, nrow = 1)

```
Diabetic women have a higher BMI but we cannot clearly distingush if to be or not to be diabetic only with this predictor.   
```{r Diabetes pedigree function stratified by outcome, fig.height=2, fig.width=6, fig.align='center'}

# Diabetes Pedigree Function density plot
P1 <- diabetes %>% 
  ggplot(aes(DiabetesPedigreeFunction, fill = Outcome))+ 
  geom_density(alpha= 0.6) +
  labs (x="Diabetes pedigree function")

# Diabetes Pedigree Function box plot
P2 <- diabetes %>% 
  ggplot(aes(Outcome, DiabetesPedigreeFunction, fill = Outcome))+ 
  geom_boxplot () +
  labs (y="Diabetes pedigree")

# Plots printout
ggarrange(P1, P2,
          ncol = 2, nrow = 1)

```
We can observe that diabetic women have a slightly higher Diabetis Pedigree Function.   
```{r Age stratified by outcome, fig.height=2, fig.width=6, fig.align='center'}

# Age density plot
P1 <- diabetes %>% 
  ggplot(aes(Age, fill = Outcome))+ 
  geom_density(alpha= 0.6) +
  labs (x="Age in years")

# Diabetes Pedigree Function box plot
P2 <- diabetes %>% 
  ggplot(aes(Outcome, Age, fill = Outcome))+ 
  geom_boxplot () +
  labs (y="Age")

# Plots printout
ggarrange(P1, P2,
          ncol = 2, nrow = 1)

```

Diabetic women seems to be older but this predictor doesn't clearly split diabetic and non diabetic women.   

\ 
\  

The following graph shows the proportion of positive and negative classes, where 0 means non-diabetic and 1 means diabetic case.

```{r Diabetic cases proportion, fig.height=2, fig.width=3, fig.align='center'}

# Bar plot to see the Outcome proportion
diabetes %>% 
  ggplot(aes(Outcome, fill=Outcome))+ 
  geom_bar()

```

We can clearly see that the dataset **prevalence** is the **positive class 0**, that means more cases without diabetes.
    
      
\newpage
**Correlations**

We will now have a look at the correlation of each pair of features. This table shows the correlation values.  
\  


```{r Correlations, fig.height=10, fig.width=20, fig.align='center'}

# Calculating variables correlation
Correlations <- cor(data.matrix(diabetes))

# Writting reduced descriptions of each variable for a better table printout
Abreviation <- c("Pregnancies","Glucose","Blood P","Skin T","Insulin", "BMI","Pedigree","Age", "Outcome")
#Variable <- c("Pregnancies", "Glucose","BloodPressure","SkinThickness","Insulin","BMI","DiabetesPedigreeFunction","Age","Outcome")

rownames(Correlations)<- Abreviation
colnames(Correlations)<- Abreviation

# Table printout
kable(round(Correlations,4), col.names = Abreviation)

```


This heatmap shows the predictors correlations but with a visual format.

```{r Heatmap,fig.align='center'}

# Correlation heatmap
heatmap(Correlations, main="Heatmap")

```

Dark colors indicate high correlation and light colors the opposite.  
   
   
\newpage

A correlogram is a different way to see the correlation between two variables and maybe even easier to follow up.   
   
   
   
   
   
     
     

```{r variables correlogram}

# Converting the Outcome variable to numeric for the corrplot() function
diabetes$Outcome <- as.numeric (diabetes$Outcome)

# Printout of the correlogram
corrplot(cor(diabetes[, 1:9]), type = "lower", method="number")

# Setting the Outcome vriable as a factor again
diabetes$Outcome <- as.factor (diabetes$Outcome)

# Setting the right levels for the Outcome
levels(diabetes$Outcome)<- c(0,1)

```
     
    
      
      
The features with higher correlation are:

**Age - Pregnancies**  
**BMI - SkinThickenss**  
**Outcome - Glucose**  
\    
\    
\    
We see that **Glucose** is the variable with highest correlation with **Outcome**, then goes **BMI**, **Age** and **Pregnancies**.

```{r Highest correlations with Outcome}

# Sorting the variables correlation with Outcome variable
HCorr <-sort(Correlations [9,], decreasing=TRUE)

# Printout of the 4 variables with highest Outcome correlation
HCorr [2:5]

```
\newpage

Let's have a look at the relationship of the top 3 pair of highest correlated predictors.

```{r Age - Pregnancies relationship, fig.align='center', fig.height=2, fig.width=6}

# Computing the average number of pregnancies by Outcome
AvgPregnancies <- diabetes %>%
  group_by(Outcome) %>%
  summarize(AvgPregnancies=mean(Pregnancies))

# Computing the average Age by Outcome
AvgAge <- diabetes %>%
  group_by(Outcome) %>% 
  summarize(AvgAge=mean(Age))

# Plot "Age - Number of Pregnancies" graph relationship
P1 <- diabetes %>% 
  ggplot(aes(Age, Pregnancies))+ 
  geom_point(size = 0.5)+
  geom_smooth(method=loess, formula = y ~ x)+
  geom_vline(data = AvgAge, aes(xintercept = AvgAge,color = Outcome))+
  geom_hline(data = AvgPregnancies, aes(yintercept = AvgPregnancies,color = Outcome))+
  labs (x="Age in years", y="Number of pregnancies")+
  ggtitle("Age - Pregnancies")

# Plot "Age - Number of Pregnancies" stratified by Outcome
P2 <-diabetes %>% 
  ggplot(aes(Age, Pregnancies, color=Outcome))+ 
  geom_point(size = 0.5)+
  labs (x="Age in years", y="Number of pregnancies")+
  ggtitle("Age - Pregnancies stratified")

# Printing out plots
ggarrange(P1,P2,
          ncol = 2, nrow = 1)

```

The blue line shows that the number of pregnancies increase with age. One could also expect that after fertility period, the number of pregnancies keep flat. However, we see that this value goes down and at the same time we have less data in the scatter plot with an increase of confidence interval. Several reasons could be behind this number of pregnancies drop, like lower natality rates because of any crisis, women with a higher number of pregnancies has a lower life expectancy or sample data too small and by chance these values are low.  

Red and green lines indicate the average value of each predictor stratified by outcome. The average age of diabetic people is higher as well as the average number of pregnancies.   

No clear boundary can be drawn that separates non diabetic and diabetic women based on number of Pregnancies vs Age.   


```{r BMI - Skin thickness relationship, fig.align='center', fig.height=2, fig.width=6}

# Computing the average BMI value by Output
AvgBMI <- diabetes %>%
  group_by(Outcome) %>% 
  summarize(AvgBMI=mean(BMI))
 
# Computing the average Skin thickness value by Output                     
AvgSkinT <- diabetes %>% 
  group_by(Outcome) %>% 
  summarize(AvgSkinT=mean(SkinThickness))

# Plot "BMI - Skin Thickness" graph relationship
P1 <- diabetes %>% 
  ggplot(aes(BMI , SkinThickness ))+
  geom_point(size = 0.5)+
  geom_smooth(method=loess, formula = y ~ x)+
  geom_vline(data = AvgBMI, aes(xintercept = AvgBMI,color = Outcome))+
  geom_hline(data = AvgSkinT, aes(yintercept = AvgSkinT,color = Outcome))+
  labs (x="BMI - Body Mass Index", y="Skin thickness in mm")+
  ggtitle("BMI - Skin thickness")

# Plot "BMI - Skin Thickness" stratified by Outcome
P2 <-diabetes %>% 
  ggplot(aes(BMI, SkinThickness, color=Outcome))+ 
  geom_point(size = 0.5)+
  labs (x="BMI - Body Mass Index", y="Skin thickness in mm")+
  ggtitle("BMI - Skin thickness stratified")

# Printing out plots
ggarrange(P1,P2,
          ncol = 2, nrow = 1)

```
This graph shows that the higher the BMI is, the thicker the skin is.  

We can also observe that diabetic people, as average, roughly have a BMI 20% higher than non diabetics.   

Non diabetic women have lower BMI and skin thickness.   

\newpage
   
   

```{r Glucose - Diabetic probability, fig.align='center', fig.height=2, fig.width=6}

# Computing the average Glucose level by Output
AvgGlucose <- diabetes %>%
  group_by(Outcome) %>% 
  summarize(AvgGlucose=mean(Glucose))

# Plot "Glucose - Output" relationship
P1 <- diabetes %>%
  group_by(Glucose) %>% 
  summarize (avg=mean(Outcome==1)) %>%
  ggplot(aes(Glucose,avg))+geom_point()+
  geom_smooth(method=loess, formula = y ~ x)+
  geom_vline(data = AvgGlucose, aes(xintercept = AvgGlucose,color = Outcome))+
  labs (x="Glucose", y="Diabetic probability")+ 
  ggtitle("Glucose - Outcome")

# Plot "BMI - Skin Thickness" stratified by Outcome
P2 <-diabetes %>% 
  ggplot(aes(Glucose, Outcome, color=Outcome))+ 
  geom_point(size = 0.5)+
  labs (x="Glucose", y="Outcome")+
  ggtitle("BMI - Outcome stratified")

# Printing out plots
ggarrange(P1,P2,
          ncol = 2, nrow = 1)

# Printout of the average Glucose table by Output
AvgGlucose %>% 
  knitr::kable() 

```
   

The blue line shows the probability to be diabetic as per the glucose level.   

Diabetics average glucose level is 30% higher than non diabetics.
   
 
   


## 2.2 Data normalization

Our dataset contains features with different units and dimensions. Some of the algorithms, not all, are based on predictors distance. We do not want our models to be affected by the magnitude of these variables. The algorithm should not be biased towards variables with higher magnitude. To overcome this problem, we will normalize the variables. 

We will calculate the mean and standard deviation of the variable, and then, for each observation we will substract the mean and then divide by the standard deviation of that variable.   

\begin{center}
$z_{i} = \frac {x_{i}-\overline{x}}{\sigma }$
\end{center}

```{r Data normalization}

######################
# Data normalization #
######################

# Declaration of function to normalize data
normalize <- function(x) 
  {
  return ((x - mean(x)) / sd(x))
  }

# Data normalization
diabetes$Pregnancies <-normalize(diabetes$Pregnancies)

diabetes$Glucose <-normalize(diabetes$Glucose)

diabetes$BloodPressure <-normalize(diabetes$BloodPressure)

diabetes$SkinThickness <-normalize(diabetes$SkinThickness)

diabetes$Insulin <-normalize(diabetes$Insulin)

diabetes$BMI <-normalize(diabetes$BMI)

diabetes$DiabetesPedigreeFunction <-normalize(diabetes$DiabetesPedigreeFunction)

diabetes$Age <-normalize(diabetes$Age)

```
   
   
   
   
## 2.3 Data splitting

We will split the dataset in **train**, **test** and **validation** set to build different machine learning algorithms.

The **train set** is the sample of data to train the model. The **test set** is the data used for fine-tuning some models. The **validation set** simulates a set of theoretical real values which is only used once to see how the algorithm performs with unkknown data.

Our dataset is not large, but we only have to predict a two class categorical outcome. Deciding the partition proportion of each set is a compromise. A large training set should let us build a more accurate model but there will be less data left for cross-validation and that will not help accuracy because we will end up with with a train and test set with different data distributions. A model trained on a vastly different data distribution than the test set will perform inferiorly with the validation set.

\newpage 
We will create the **validation_set** as 10% of the total diabetes dataset.

```{r Creating the validation set}

##################
# Data splitting #
##################

# if using R 3.5 or earlier, use `set.seed(1)` instead
set.seed(1, sample.kind="Rounding")

# The validation set is 10% of the diabetes dataset
test_index <- createDataPartition(y = diabetes$Outcome, times = 1, p = 0.1, list = FALSE)

temp <- diabetes[-test_index,]

validation_set <- diabetes[test_index,]
 
```

The **train_set** set will be 80% and the **test_set** set will be 20% of the 90% of the diabetes dataset.
```{r Creating the train and test set}

# if using R 3.5 or earlier, use `set.seed(1)` instead
set.seed(1, sample.kind="Rounding")

# The train set will be 80% and the test set 20% of the 90% diabetes datatset
test_index <- createDataPartition(y = temp$Outcome, times = 1, p = 0.2, list = FALSE)

 train_set <- temp[-test_index,]
 
 test_set <- temp[test_index,]
 
```

The **validation_set** has 77 observations.

```{r Validation_set structure}

# Structure of the validation set
str (validation_set)

```
The **train_set** has 552 observations.
```{r train_set structure}

# Structure of the train set
str (train_set)

```
The **test_set** has 139 observations.
```{r test_set structure}

# Structure of the test set
str (test_set)

```
All together we have 768 observations, the same number as the original diabetes dataset.

\newpage

## 2.4 Modelling approaches

There are several common techniques to develop machine learning algorithms. In our case we will compare the performance of four different models.

## 2.4.1 Logistic regression model

The **logistic regression** method is a classification algorithm to assaign observations to a discrete set of classes. The logistic regression can help us to predict the risk to be diabetic as the predictions are discrete.

This is the overall accuracy result obtained after predicting the outcome with the logistic regression model fitting.

```{r Logistic regression model}

########################
# Modelling approaches #
########################

#############################
# Logistic regression model #
#############################

# Fitting the logistic regression model
fit_glm <- train(Outcome ~ ., method = "glm", 
    data = train_set)

```

```{r Creation of results table}

# Checking the overall accuracy with the logistic regression model
acc_glm <- confusionMatrix(predict(fit_glm, test_set), test_set$Outcome)$overall[["Accuracy"]]

# Saving the model accuracy 
model_accuracy<- data_frame(Method = "Logistic regression model", Accuracy = acc_glm)

# Creating the results table with the test set
accuracy_results <- data_frame(Method = "Logistic regression model", Accuracy = acc_glm)

# Printout of the model accuracy
model_accuracy %>% 
  knitr::kable()

```
    
The predictor's p-value indicate the meaning variables for the model. A p-value lower than 0.05 indicates that changes in the predictor's value are related to changes in the response variable.
```{r logistic regression model p-values}

# Printout of logistic regression model to check the p-value
summary(fit_glm)

```

\newpage   



## 2.4.2 KNN model

The **KNN** (K-nearest neighbors) model is an algorithm used in machine learning that works with predictors distances.

The **K** parameter indicates the number of nearest neighbors the model considers. This parameter has to be optimized with cross-validation to obtain the highest possible accuracy.

```{r KNN model}

#############
# KNN model #
#############

# if using R 3.5 or earlier, use `set.seed(1)` instead
set.seed(2, sample.kind="Rounding")

# Fitting the KNN model
fit_knn <- train(Outcome ~ ., method = "knn", tuneGrid = data.frame(k = seq(10, 80, 2)),
    data = train_set)

```
In the next graph we can see that the best K value that generates the highest accuracy is 46.

```{r K graph, fig.height=3, fig.width=3, fig.align='center'}

# Plotting the model to see the best K value
ggplot(fit_knn, highlight = TRUE)

```

This is the overall accuracy of the KNN model.

```{r Accuracy of the KNN model}

# Checking the overall accuracy with the KNN model
acc_knn <- confusionMatrix(predict(fit_knn, test_set), test_set$Outcome)$overall[["Accuracy"]]

# Saving the model accuracy
model_accuracy<- data_frame(Method = "KNN model", Accuracy = acc_knn)

# Adding the model accuracy to the results table
accuracy_results <- bind_rows(accuracy_results,
                          data_frame(Method="KNN Model",  
                                     Accuracy = acc_knn ))

# Printout of the model accuracy
model_accuracy %>% 
  knitr::kable()

```


\newpage 

## 2.4.3 CART model

The **CART** (Classification And Regression Tree) model is a predictive model, which explains how an outcome variable's values can be predicted based on other values. A CART output is a decision tree where each fork is a split in a predictor variable and each end node contains a prediction for the outcome variable.  
  
We will use cross-validation to choose **CP** (Complexity Prameter) value. The complexity parameter (CP) is used to control the size of the decision tree and to select the optimal tree size. If the cost of adding another variable to the decision tree from the current node is above the value of CP, then tree building does not continue.  





```{r CART model, fig.height=3, fig.width=3, fig.align='center'}

##############
# CART model #
##############

# Fitting the CART model
fit_CART <- train(Outcome ~ ., method = "rpart",tuneGrid = data.frame(cp = seq(0, 0.05, len = 25)),
    data = train_set)

# Plotting the CART model to see the best CP
ggplot(fit_CART, highlight = TRUE)

```
This is the best setting for the CP.

```{r Best CP value}

# Printout of the best CP value
fit_CART$bestTune

```
And this is the decision tree structure, which is based in 2 predictors only.
```{r Decision tree scheme, fig.height=3, fig.width=3, fig.align='center'}

# Plotting the CART tree
plot(fit_CART$finalModel, margin = 0.1)
text(fit_CART$finalModel, cex = 0.75)

```
\newpage
The variable importance of the CART model is as follows.
```{r variable importance with CART model}

# Printout of the variables importance
varImp(fit_CART)

```


And this is the obtained overall Accuracy with the CART method.

```{r Accuracy of the CART model}

# Checking the overall accuracy with the CART model
acc_CART <- confusionMatrix(predict(fit_CART, test_set), test_set$Outcome)$overall[["Accuracy"]]

# Saving the model accuracy
model_accuracy<- data_frame(Method = "CART model", Accuracy = acc_CART)

# Adding the model accuracy to the results table
accuracy_results <- bind_rows(accuracy_results,
                          data_frame(Method="CART Model",  
                                     Accuracy = acc_CART ))

# Printout of the model accuracy
model_accuracy %>% 
  knitr::kable()

```
   
   
\newpage

## 2.4.4 Random forest model

The **Random forest** model is an algorithm that randomly creates several small decission trees that all together make one forest. The solutions comes from a collection of decision model to improve acuracy.  

```{r Random Forest mode, fig.height=3, fig.width=3, fig.align='center'}

#######################
# Random Forest model #
#######################

# if using R 3.5 or earlier, use `set.seed(1)` instead
set.seed(2, sample.kind="Rounding")

# Parameters for cross-validation
control <- trainControl(method="cv", number=10, p=0.9)

metric <- "Accuracy"

mtry <- seq(10, 100, 10)

tunegrid <- expand.grid(.mtry=mtry)

# Fitting the Random Forest model
fit_RF <- train(Outcome~., data=train_set, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)

# Plotting cross-validation best parameters for accuracy
ggplot(fit_RF, highlight = TRUE)

```
We now check the variable importance of the model with the **varImp()** function of the caret package.

```{r Variable importance with Random Forest model}

# Printout of variables importance
varImp(fit_RF)

```
**Glucose** seems to be the most important variable of this Random Forest model followed by **BMI**.
```{r Accuracy of the Random Forest model}

# Checking the overall accuracy with the Random Forest model
acc_RF <- confusionMatrix(predict(fit_RF, test_set), test_set$Outcome)$overall[["Accuracy"]]

# Saving the model accuracy
model_accuracy<- data_frame(Method = "Random Forest model", Accuracy = acc_RF)

# Adding the model accuracy to the results table
accuracy_results <- bind_rows(accuracy_results,
                          data_frame(Method="Random Forest Model",  
                                     Accuracy = acc_RF ))

# Printout of the model accuracy
model_accuracy %>% 
  knitr::kable()

```



\newpage


## 3.1 Results

This is the summary of the 4 different models results sorted by overall accuracy when training the models.


```{r Summary of accuracies}

###########
# Results #
###########

# Sorting the models by overall accuracy
accuracy_results <- accuracy_results %>%
  arrange(desc(Accuracy))

# Printout of the overall accuracy models table
accuracy_results %>% 
  knitr::kable()

```
    
We will now assess the performance of the four models with hypothetical real results when using them with the **validation set**.  


```{r overall accuracy with validation set}

# Checking the overall accuracy with the logistic regression model
acc_glm <- confusionMatrix(predict(fit_glm, validation_set), validation_set$Outcome)$overall[["Accuracy"]]

# Saving the model accuracy
model_accuracy<- data_frame(Method = "Logistic regression model", Accuracy = acc_glm)

# Creating the results table with the validation set
accuracy_results <- data_frame(Method = "Logistic regression model", Accuracy = acc_glm)

# Checking the overall accuracy with the KNN model
acc_knn <- confusionMatrix(predict(fit_knn, validation_set), validation_set$Outcome)$overall[["Accuracy"]]

# Saving the model accuracy
model_accuracy<- data_frame(Method = "KNN model", Accuracy = acc_knn)

# Adding the model accuracy to the results table
accuracy_results <- bind_rows(accuracy_results,
                          data_frame(Method="KNN Model",  
                                     Accuracy = acc_knn ))

# Checking the overall accuracy with the CART model
acc_CART <- confusionMatrix(predict(fit_CART, validation_set), validation_set$Outcome)$overall[["Accuracy"]]

# Saving the model accuracy
model_accuracy<- data_frame(Method = "CART model", Accuracy = acc_CART)

# Adding the model accuracy to the results table
accuracy_results <- bind_rows(accuracy_results,
                          data_frame(Method="CART Model",  
                                     Accuracy = acc_CART ))

# Checking the overall accuracy with the Random Forest model
acc_RF <- confusionMatrix(predict(fit_RF, validation_set), validation_set$Outcome)$overall[["Accuracy"]]

# Saving the model accuracy
model_accuracy<- data_frame(Method = "Random Forest model", Accuracy = acc_RF)

# Adding the model accuracy to the results table
accuracy_results <- bind_rows(accuracy_results,
                          data_frame(Method="Random Forest Model",  
                                     Accuracy = acc_RF ))

# Sorting the models by overall accuracy
accuracy_results <- accuracy_results %>% arrange(desc(Accuracy))

# Printout of the overall accuracy models table with the validation set
accuracy_results %>% 
  knitr::kable()

```
   
       
    
## 3.2 Discussion  

We see that the model that better performs in terms of overall accuracy when developing the algorithm is the **Random forest model**. However, the best performing model with the **validation set** is the **KNN model**.  
  
We can see that the **overall accuracy** of the different models is different when using the **validation set** and also the order of the most accurate models.

The reason of the different model performance when training and when using the validation set can be the small size of the Pima Indians Diabetes dataset.  

We can also see that the results of the logistic regression model and the CART model are the same. We have to confirm if this is a casuality or not.  

This is the result when comparing the prediction on the validation set with the logistic regression model and CART model with **identical()** function.  

```{r prediction with validation set and the logistic regression model}

##############
# Discussion #
##############

# Checking if predicted results using Logistic Regression model and CART model are the same
identical (predict(fit_glm, validation_set),predict(fit_CART, validation_set))

```

The identical() function tells us that prediction results are not the same. So, let's have a look at the written results.  

\newpage

These are the predicted values of the validation set using the **logistic regression** model.  

```{r prediction with validation set and logistic regression model}

# Printout of the predicted values with the Logistic Regression model
predict(fit_glm, validation_set)

```

These are the predicted values of the validation set using the **CART** model.

```{r prediction with validation set and CART model}

# Printout of the predicted values with the CART model
predict(fit_CART, validation_set)

```

Now we can visually appreciate that the matrices are not the same.  
   
   
   
   
   


Now we will have a look at the Confussion Matrix of both models.

**Logistic regression** model Confussion Matrix.  


```{r Logistic regression model Confussion Matrix}

# Printout of the Confussion Matrix using the Logistic Regression model
confusionMatrix(predict(fit_glm, validation_set), validation_set$Outcome)

```
\newpage

**CART** model Confussion Matrix.  


```{r CART model Confussion Matrix}

# Printout of the Confussion Matrix using the CART model
confusionMatrix(predict(fit_CART, validation_set), validation_set$Outcome)

```
   
   
Now we can perfectly see that despite the predictive models are different, the overall accuracy in both cases is the same.  
   
However, sensitivity and specificity are quite different, being the CART model specificity specially poor. More similar is the balanced accuracy of both models.



\newpage

## 4. Conclusions

The four predictive models we have build perform different when training and when being used with a hypothetical real case.  

The size of the dataset conditions the good performance of the predective models.  

The most meaningful predictors are different for each type of model, but **Glucose** and **BMI** are always in the top 2. This conclusion has been proofed except for the KNN model as in this case there is no way to check the variable importance.  

**Glucose** and **BMI** are the predictors with highest correlation with the **Outcome**, which is the fact to be or not to be diabetic. Higher values increase the probability to be diabetic.  

## Future work  

It would be interesting to study if other predictive models based on different algorithms perform in a similar way and are also sensitive to the size of the dataset.  

It is also possible to find better tuning parameters for the used models.  

It is also an interesting exercice to study which of the predictive models shows a higher overall accuracy after a Montecarlo simulation that generates many train and test set creation. We could then choose the best performing model. The problem is that this research task can consume a lot of time.     

We can repeat the previous exercice but this time with different proportions of the train and test set and observe which proportion benefits the predictive overall accuracy of the models.   






   
   